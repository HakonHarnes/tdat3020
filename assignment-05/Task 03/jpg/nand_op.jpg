import torch
import matplotlib.pyplot as plt
import csv
import torch


x_train = torch.FloatTensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
y_train = torch.FloatTensor([[1.0], [1.0], [1.0], [0.0]]).reshape(-1, 1)

torch.reshape(x_train, (-1,2))

class LinearRegressionModel:
    def __init__(self):
        # Model variables
        self.W = torch.tensor([[0.00], [0.00]], requires_grad=True)  # requires_grad enables calculation of gradients
        self.b = torch.tensor([[1.00]], requires_grad=True)

    def logits(self, x):
        return x @ self.W + self.b 

    # Predictor
    def f(self, x):
        return torch.sigmoid(self.logits(x)) 

    # Uses Mean Squared Error
    def loss(self, x, y):
        return torch.nn.functional.binary_cross_entropy_with_logits(self.logits(x), y)


model = LinearRegressionModel()

# Optimize: adjust W and b to minimize loss using stochastic gradient descent
optimizer = torch.optim.SGD([model.b, model.W], 0.015)
for epoch in range(100000):
    model.loss(x_train, y_train).backward()  # Compute loss gradients
    optimizer.step()  # Perform optimization by adjusting W and b,
    # similar to:
    # model.W -= model.W.grad * 0.01
    # model.b -= model.b.grad * 0.01

    optimizer.zero_grad()  # Clear gradients for next step


print("W = %s \nb = %s \nloss = %s" % (model.W, model.b, model.loss(x_train, y_train)))


fig = plt.figure("Logistic regression: the logical OR operator")

plot1 = fig.add_subplot(111, projection='3d')

plot1_f = plot1.plot_wireframe(torch.tensor([[]]), torch.tensor([[]]), torch.tensor([[]]), color="green", label="$y=f(x)=\\sigma(xW+b)$")

plot1.plot(x_train[:, 0].squeeze(),
           x_train[:, 1].squeeze(),
           y_train[:, 0].squeeze(),
           'o',
           label="$(\\hat x_1^{(i)}, \\hat x_2^{(i)},\\hat y^{(i)})$",
           color="blue")

plot1_info = fig.text(0.01, 0.02, "")

plot1.set_xlabel("$x_1$")
plot1.set_ylabel("$x_2$")
plot1.set_zlabel("$y$")
plot1.legend(loc="upper left")
plot1.set_xticks([0, 1])
plot1.set_yticks([0, 1])
plot1.set_zticks([0, 1])
plot1.set_xlim(-0.25, 1.25)
plot1.set_ylim(-0.25, 1.25)
plot1.set_zlim(-0.25, 1.25)

table = plt.table(cellText=[[0, 0, 0], [0, 1, 0], [1, 0, 0], [1, 1, 0]],
                  colWidths=[0.1] * 3,
                  colLabels=["$x_1$", "$x_2$", "$f(x)$"],
                  cellLoc="center",
                  loc="lower right")



plot1_f.remove()
x1_grid, x2_grid = torch.meshgrid(torch.linspace(-0.25, 1.25, 10), torch.linspace(-0.25, 1.25, 10))
y_grid = torch.empty([10, 10])
for i in range(0, x1_grid.shape[0]):
    for j in range(0, x1_grid.shape[1]):
            a = [[x1_grid[i, j], x2_grid[i, j]]]
            x_plot = torch.FloatTensor(a)
            y_grid[i, j] = model.f(x_plot)
plot1_f = plot1.plot_wireframe(x1_grid.detach(), x2_grid.detach(), y_grid.detach(), color="green")

plot1_info.set_text(
    "$W=\\genfrac{[}{]}{0}{}{%.2f}{%.2f}$\n$b=[%.2f]$\n$loss = -\\frac{1}{n}\\sum_{i=1}^{n}\\left [ \\hat y^{(i)} \\log\\/f(\\hat x^{(i)}) + (1-\\hat y^{(i)}) \\log (1-f(\\hat x^{(i)})) \\right ] = %.2f$"
    % (model.W[0, 0], model.W[1, 0], model.b[0, 0], model.loss(x_train, y_train)))

table._cells[(1, 2)]._text.set_text("${%.1f}$" % model.f(x_train[0]))
table._cells[(2, 2)]._text.set_text("${%.1f}$" % model.f(x_train[1]))
table._cells[(3, 2)]._text.set_text("${%.1f}$" % model.f(x_train[2]))
table._cells[(4, 2)]._text.set_text("${%.1f}$" % model.f(x_train[3]))

fig.canvas.draw()

plt.show()
